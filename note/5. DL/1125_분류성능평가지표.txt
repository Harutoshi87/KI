- confusion matrix(혼동행렬, 교차표, 분류성능 평가지표)
           예측0    예측1
실제0      TN        FP
실제1      FN        TP
T:모델이 맞췄을 때 / F:모델이 틀렸을 때
P:예측값이 1        / N: 예측값이 0

accuracy(정확도) = (TN+TP)/(TN+FP+FN+TP)
precision(정밀도) = TP / 예측을 1이라고 한 수(FP+TP) -> 예측한 Positive 중 실제 Positive 비율
	(예측값기준)
recall(재현율=민감도) = TP / 실제값이 1인 수(FN+TP) -> 실제 Positive 중에서 모델이 맞게 예측한 비율
	(실제값기준)
F1 score = recall과 precision의 조화평균
	    F1 score =2* (precision*recall)/(precision+recall)
	*  F1 score는 평가 단계에서 활용

트레이드 오프 관계
Precision을 높이려면 -> 1일 확률이 높은 것만 Positive로 예측함 -> recall이 낮아질 수 있음
recall을 높이려면 -> 조금이라도 가능성이 있으면 Positive로 예측-> precision이 낮아질 수 있음

ex1. 암 진단 AI
recall ↑ : 환자를 놓지지 않기 위해 의심이 가면 다 positive로 예측
	(결과) 환자가 아닌데 재검사 받는 경우

ex2. 금융사기 탐지 AI
recall ↑: 사기일 가능성이 조금만 있어도 다 사기로 예측
	(결과) 고객 불만 ↑  
precision ↑ : 확실하게 사기일 경우만 사기로 예측. 
	(결과) 회사의 비용 ↑


# 교차표(혼동매트릭스, 성능평가지표) 그리기 : 테스트셋의 실제값과 예측값
y_hat = (model.predict(X_test) > 0.5).astype(int)
y_test.shape, y_hat.shape # 실제값(1차원), 예측값(2차원)

y_hat.reshape(-1).shape #혼동행렬(Confusion Matrix) 같은 함수는 1차원 벡터 형태를 요구

TN=0; FP=0; FN=0; TP=0;
for y, h in zip(y_test, y_hat.reshape(-1)):
    # print(y, h)
    if y==0 and h==0:
        TN += 1  # TN갯수 하나 증가
    elif y==0 and h==1:
        FP += 1
    elif y==1 and h==0:
        FN += 1
    else:
        TP += 1
print(TN, FP)
print(FN, TP)

ctab = pd.crosstab(y_test, y_hat.reshape(-1)) # y_hat에러남
ctab.index.name = '실제값'
ctab.columns.name = '예측값'
ctab

pd.crosstab(y_test, # 실제값
           y_hat.reshape(-1), # 예측값
           rownames=['실제값'], # index이름
           colnames=['예측값']) # column이름  : 결과가 데이터 프레임인 교차표


from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_hat) # y_hat.reshape(-1) 둘다 가능
# 결과가 numpy배열인 혼동행렬



