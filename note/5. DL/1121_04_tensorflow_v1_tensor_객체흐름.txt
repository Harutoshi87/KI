1. tensorflow v2.xx ì—ì„œ v1 ì‚¬ìš©í•˜ê¸°
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() # tensorflow v2 ë¹„í™œì„±í™”í•˜ê³  v1 í™œì„±í™”
import numpy as np
import pandas as pd

tensorflow:
ë°ì´í„° íë¦„ ê·¸ë˜í”„(tensor ê°ì²´ì˜ íë¦„)ì„ ì‚¬ìš©í•˜ëŠ” ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
ê·¸ë˜í”„ëŠ” node(ì—°ì‚°)ì™€ edgeë¡œ êµ¬ì„±
sess = tf.Session()ì„ ì´ìš©í•˜ì—¬ ì‹¤í–‰ í™˜ê²½ ì¡°ì„±
sess.run()ì„ í†µí•˜ì—¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸

# 1ë‹¨ê³„ : tensor(ìƒìˆ˜ node í•˜ë‚˜) ì •ì˜
node1 = tf.constant('Hello, Tensorflow')
# 2ë‹¨ê³„ : Session(ì—°ì‚°ì„ ì‹¤í–‰í•˜ëŠ” í™˜ê²½) ìƒì„±
sess = tf.Session()
# 3ë‹¨ê³„ : ì‹¤ì œ ì‹¤í–‰ ë° ì¶œë ¥
print(sess.run(node1))
print(sess.run(node1).decode())

# ê°„ë‹¨í•œ ì—°ì‚° tensor ê·¸ë˜í”„
# 1. ê·¸ë˜í”„ ì •ì˜
node1 = tf.constant(10, dtype=tf.float16)
node2 = tf.constant(20, dtype=tf.float16)
node3 = tf.add(node1, node2)
# 2. Session ìƒì„±
sess = tf.Session()
# 3. Session ì‹¤í–‰ & ê²°ê³¼
print(sess.run([node1, node2, node3]))
[10.0, 20.0, 30.0]

# íƒ€ì… ë³€ê²½
node1 = tf.constant(np.array([1,2,3]), dtype=tf.int16)
node2 = tf.cast(node1, dtype=tf.float32)
sess = tf.Session()
print(sess.run([node1, node2]))
[array([1, 2, 3], dtype=int16), array([1., 2., 3.], dtype=float32)]

# í‰ê· ê°’ ê³„ì‹¼ : tf.reduce_mean()
data = np.array([1.,2.,3.,4.])
m = tf.reduce_mean(data)
sess = tf.Session()
print(sess.run(m))
2.5

# tf.random_normal([n]) : í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 1ì¸ ë‚œìˆ˜ nê°œ ë°œìƒ
w = tf.random_normal([3]) 
sess = tf.Session()
sess.run(w)
array([-0.6306145 ,  0.21123382, -0.35345677], dtype=float32)

# ë³€ìˆ˜ node
w = tf.Variable(tf.random_normal([1]))
sess = tf.Session()
sess.run(tf.global_variables_initializer()) # ë³€ìˆ˜ ì´ˆê¸°í™” : ì´ê²ƒì„ ì‹¤í–‰í•´ì•¼ ìƒê¸° ë‚œìˆ˜ 1ê°œê°€ wì— ë³€ìˆ˜ë¡œ ë“¤ì–´ê°
sess.run(w)

# predictë¥¼ ìœ„í•œ placeholder ì´ìš©
placeholder : ì™¸ë¶€ì—ì„œ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” node
x = tf.placeholder(dtype=tf.float32)
H = 1 * x + 1
sess = tf.Session()
sess.run([x, H], feed_dict={x : np.array([40,50])})
[array([40., 50.], dtype=float32), array([41., 51.], dtype=float32)]

# Scale ì¡°ì •
scaleì´ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì˜ íšŒê·€ë¶„ì„ êµ¬í˜„(scale ì¡°ì • O)
scale ì¡°ì • ë°©ë²• : ëª¨ë“  ë°ì´í„°ë¥¼ ì¼ì • ë²”ìœ„ ë‚´ë¡œ ì¡°ì •
normalization(ì •ê·œí™”) : ëª¨ë“  ë°ì´í„°ë¥¼ 0~1 ì‚¬ì´ë¡œ ì¡°ì • 
normalization = X - Xmin / Xmax - Xmin
  * ìœ„ì˜ ì‹ë³´ë‹¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ì²œ (sklearn.preprocessing.MinMaxScaler)
standardization(í‘œì¤€í™”) : ë°ì´í„°ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ì¡°ì •
standardization = X - Xmean / Xstd
  * ìœ„ì˜ ì‹ë³´ë‹¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ì²œ (sklearn.preprocessing.StandardScaler)

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì“°ì§€ ì•Šê³  ì •ê·œí™”
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
# X_data.min(), X_data.max(), X_data.mean(), X_data.std()
norm_scaled_x_Data = (x_Data - min(x_Data)) / (max(x_Data) - min(x_Data))
norm_scaled_y_Data = (y_Data - min(y_Data)) / (max(y_Data) - min(y_Data))
norm_scaled_x_Data, norm_scaled_y_Data
(array([0.        , 0.11111111, 0.44444444, 0.77777778, 1.        ]),
 array([0.        , 0.11111111, 0.7       , 0.83333333, 1.        ]))

# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™” : ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë„£ê¸° ìœ„í•´ì„œëŠ” ní–‰ 1ì—´ë¡œ reshape í•„ìš”!
x_Data = x_Data.reshape(-1, 1)
y_Data = y_Data.reshape(-1, 1)
from sklearn.preprocessing import MinMaxScaler # class
scaler_x = MinMaxScaler() # ë…ë¦½ë³€ìˆ˜ xë¥¼ ì •ê·œí™”ì‹œí‚¬ ê°ì²´
scaler_x.fit(x_Data)
norm_scaled_x_Data = scaler_x.transform(x_Data)
scaler_y = MinMaxScaler() # ì¢…ì†ë³€ìˆ˜ yë¥¼ ì •ê·œí™”ì‹œí‚¬ ê°ì²´
norm_scaled_y_Data = scaler_y.fit_transform(y_Data)
norm_scaled_x_Data, norm_scaled_y_Data
(array([[0.        ],
        [0.11111111],
        [0.44444444],
        [0.77777778],
        [1.        ]]),
 array([[0.        ],
        [0.11111111],
        [0.7       ],
        [0.83333333],
        [1.        ]]))

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì“°ì§€ ì•Šê³  í‘œì¤€í™”
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
stan_scaled_x_Data = (x_Data - x_Data.mean()) / x_Data.std()
stan_scaled_y_Data = (y_Data - y_Data.mean()) / y_Data.std()
np.column_stack([x_Data, norm_scaled_x_Data, stan_scaled_x_Data])
array([[ 1.        ,  0.        , -1.22474487],
       [ 2.        ,  0.11111111, -0.93313895],
       [ 5.        ,  0.44444444, -0.05832118],
       [ 8.        ,  0.77777778,  0.81649658],
       [10.        ,  1.        ,  1.39970842]])
np.column_stack([y_Data, norm_scaled_y_Data, stan_scaled_y_Data])
array([[ 5.        ,  0.        , -1.32373476],
       [15.        ,  0.11111111, -1.04563922],
       [68.        ,  0.7       ,  0.42826713],
       [80.        ,  0.83333333,  0.76198177],
       [95.        ,  1.        ,  1.17912508]])
# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì¤€í™” : ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë„£ê¸° ìœ„í•´ì„œëŠ” ní–‰ 1ì—´ë¡œ reshape í•„ìš”!
x_Data = x_Data.reshape(-1, 1)
y_Data = y_Data.reshape(-1, 1)
from sklearn.preprocessing import StandardScaler # class
scaler_x = StandardScaler() # ë…ë¦½ë³€ìˆ˜ xë¥¼ ì •ê·œí™”ì‹œí‚¬ ê°ì²´
stan_scaled_x_Data = scaler_x.fit_transform(x_Data)
scaler_y = StandardScaler() # ì¢…ì†ë³€ìˆ˜ yë¥¼ ì •ê·œí™”ì‹œí‚¬ ê°ì²´
stan_scaled_y_Data = scaler_y.fit_transform(y_Data)
stan_scaled_x_Data, stan_scaled_y_Data

# ìŠ¤ì¼€ì¼ ì¡°ì •ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë³µêµ¬
scaler_y.inverse_transform(stan_scaled_y_Data)
scaler_x.inverse_transform(stan_scaled_x_Data)


# scaleì´ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì˜ íšŒê·€ë¶„ì„ êµ¬í˜„(scale ì¡°ì • í›„)
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
# placeholder ì„¤ì •
x = tf.placeholder(dtype=tf.float32)
y = tf.placeholder(dtype=tf.float32)
# weight & bias
W = tf.Variable(tf.random_normal([1]))
b = tf.Variable(tf.random.normal([1]))
# Hypothesis
H = W * x + b
# cost(ì†ì‹¤í•¨ìˆ˜)
cost = tf.reduce_mean(tf.square(H-y))
# ê²½ì‚¬í•˜ê°•ë²•
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)
# Session ìƒì„± ë° ë³€ìˆ˜ ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())
# í•™ìŠµ(scale ì¡°ì • í›„)
for step in range(1, 6001):
    _, cost_val = sess.run([train, cost], feed_dict={x:norm_scaled_x_Data,
                                                     y:norm_scaled_y_Data})
    if step %300==0:
        print('{}/6000ë²ˆì§¸ cost:{}'.format(step, cost_val))



TensorFlow **v1 ìŠ¤íƒ€ì¼(=tf.compat.v1)** ì„ *ì¼ë¶€ëŸ¬* ë°°ìš°ëŠ” ëª©ì ì€ ì•„ë˜ì²˜ëŸ¼ **êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì´í•´ë¥¼ ì–»ê¸° ìœ„í•´ì„œ**ì´ë‹¤.
ë‹¨ìˆœíˆ â€œêµ¬ì¡° ì´í•´â€ ì •ë„ê°€ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ **ë”¥ëŸ¬ë‹ì˜ ë‚´ë¶€ ë™ì‘ì„ ê¹Šê²Œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì§€ì ë“¤**ì´ë‹¤.

---

# ğŸ”¥ TensorFlow v1 ì„ ë°°ìš°ëŠ” **êµ¬ì²´ì ì¸ ëª©ì  7ê°€ì§€**

## **1) ê·¸ë˜í”„(Graph) ì‹¤í–‰ êµ¬ì¡° ì´í•´**

* TF1ì€ **ì •ì  ê·¸ë˜í”„(static graph)** ë°©ì‹:

  ```python
  X = tf.placeholder(...)
  W = tf.Variable(...)
  H = X @ W
  sess.run(H)
  ```
* ì´ êµ¬ì¡°ë¥¼ ë°°ìš°ë©´ **ë”¥ëŸ¬ë‹ì´ â€œê³„ì‚° ê·¸ë˜í”„â€ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•œë‹¤**ëŠ” ì‚¬ì‹¤ì„ ì •í™•íˆ ì´í•´í•œë‹¤.
* TF2(Eager execution)ëŠ” ì¦‰ì‹œ ì‹¤í–‰ì´ë¼ ë‚´ë¶€ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì§ì ‘ ëŠë¼ê¸° ì–´ë ¤ì›€.

---

## **2) Sessionê³¼ Feed_dict ê°œë… ì´í•´**

* TF1ì—ì„œëŠ” ì—°ì‚°ì„ ê³„ì‚°í•˜ë ¤ë©´ ê¼­

  ```python
  sess.run(H, feed_dict={X: value})
  ```

  ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
* ì´ë¥¼ í†µí•´ **ë°ì´í„°ê°€ ê·¸ë˜í”„ì— ì–´ë–»ê²Œ ì…ë ¥ë˜ëŠ”ì§€**,
  **ë³€ìˆ˜ê°€ ê·¸ë˜í”„ ì•ˆì—ì„œ ì–´ë–»ê²Œ ìœ ì§€ë˜ëŠ”ì§€** ë°°ìš´ë‹¤.

TF2ì—ì„œëŠ” ìë™ì´ë¼ ë‚´ë¶€ íë¦„ì„ ëª¨ë¥´ê²Œ ë¨.

---

## **3) Placeholder â†’ ì…ë ¥ íŒŒì´í”„ë¼ì¸ ì´í•´**

* placeholderëŠ” **ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ì„ ë§Œë“œëŠ” ì›ë¦¬**ë¥¼ ë³´ì—¬ì¤€ë‹¤.
* TF2ëŠ” Kerasê°€ ìë™ ì²˜ë¦¬í•¨ â†’ ë‚´ë¶€ ì›ë¦¬ë¥¼ ëª¨ë¥´ê²Œ ë  ìˆ˜ ìˆìŒ.

---

## **4) Variable ì´ˆê¸°í™” ê°œë…**

* TF1ì—ì„œëŠ” ë°˜ë“œì‹œ ì§ì ‘ ë³€ìˆ˜ ì´ˆê¸°í™”:

  ```python
  sess.run(tf.global_variables_initializer())
  ```
* ì´ ê³¼ì •ì„ í†µí•´
  **ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°(W, b)ê°€ ì–´ë–»ê²Œ ìƒì„±ë˜ê³  ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°€ëŠ”ì§€** ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë‹¤.

TF2ì—ì„œëŠ” ìë™ ì²˜ë¦¬ë˜ì–´ ê°ì¶°ì ¸ ìˆìŒ.

---

## **5) Optimizer â†’ Gradient ê³„ì‚°ì˜ ì›ë¦¬ ì´í•´**

* TF1ì€ optimizer í˜¸ì¶œ ì‹œ ê·¸ë˜í”„ì— â€œgradient ë…¸ë“œâ€ê°€ ì¶”ê°€ëœë‹¤.
* ì´ë¥¼ ë³´ê³ 
  â€œê²½ì‚¬í•˜ê°•ë²•ì´ ê·¸ë˜í”„ì—ì„œ ì–´ë–»ê²Œ êµ¬í˜„ë˜ëŠ”ì§€â€
  ë‚´ë¶€ë¥¼ ë¶„ëª…í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

TF2ì—ì„œëŠ” ë°”ë¡œ `.fit()` â†’ ë‚´ë¶€ê°€ ê±°ì˜ ê°ì¶°ì§.

---

## **6) Layer ì—†ì´ ìˆœìˆ˜ ìˆ˜ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²• í•™ìŠµ**

* TF1ì—ì„œëŠ” ëª¨ë“  ëª¨ë¸ì„ ì§ì ‘ ìˆ˜ì‹ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•¨.
* ë”°ë¼ì„œ
  **ì‹ ê²½ë§ êµ¬ì¡° ìì²´ì˜ ì›ë¦¬** ë¥¼ ë§¤ìš° ëª…í™•íˆ ì´í•´í•˜ê²Œ ëœë‹¤.

ì˜ˆ:

```python
H = tf.matmul(X, W1) + b1
H = tf.nn.relu(H)
Y = tf.matmul(H, W2) + b2
```

TF2ëŠ” Dense í•˜ë‚˜ë¡œ í†µì§¸ë¡œ ì²˜ë¦¬ â†’ ë‚´ë¶€ ë™ì‘ì„ ì´í•´í•˜ê¸° ì–´ë ¤ì›€.

---

## **7) Low-level ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ê°œë… ì´í•´**

TF v1ì„ ì•Œë©´ ë‹¤ìŒ ê¸°ìˆ ì—ë„ ê°ì´ ë¹¨ë¦¬ ì˜¨ë‹¤:

* PyTorch computational graph
* JAX functional-style training loop
* ONNX ê³„ì‚° ê·¸ë˜í”„ êµ¬ì¡°
* C++ë¡œ êµ¬í˜„ëœ deep learning runtime engine

ì¦‰, **ë”¥ëŸ¬ë‹ ì—”ì§„ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ë§Œë“¤ê³  ëŒë¦¬ëŠ”ì§€** ì™„ì „íˆ ì´í•´ ê°€ëŠ¥.

---

# ğŸ‘‰ ê²°ë¡  (í•œ ì¤„ ìš”ì•½)

**TF2ëŠ” ë§¤ìš° ì‰½ì§€ë§Œ ë‚´ë¶€ë¥¼ ê°ì¶˜ë‹¤.**
**TF1ì€ ë‚´ë¶€ ì›ë¦¬ë¥¼ ëª¨ë‘ ì§ì ‘ ë§Œì§€ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ êµ¬ì¡°ë¥¼ ê¹Šê²Œ ì´í•´í•˜ê²Œ ëœë‹¤.**

ê·¸ë˜ì„œ êµìœ¡ìš©/ì—°êµ¬ìš©ìœ¼ë¡œ TF1 ìŠ¤íƒ€ì¼ì„ ì—¬ì „íˆ ë°°ìš°ëŠ” ê²ƒ.
