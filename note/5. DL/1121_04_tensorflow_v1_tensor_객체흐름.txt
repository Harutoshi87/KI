1. tensorflow v2.xx 에서 v1 사용하기
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() # tensorflow v2 비활성화하고 v1 활성화
import numpy as np
import pandas as pd

tensorflow:
데이터 흐름 그래프(tensor 객체의 흐름)을 사용하는 수치 계산 라이브러리
그래프는 node(연산)와 edge로 구성
sess = tf.Session()을 이용하여 실행 환경 조성
sess.run()을 통하여 실행 결과를 확인

# 1단계 : tensor(상수 node 하나) 정의
node1 = tf.constant('Hello, Tensorflow')
# 2단계 : Session(연산을 실행하는 환경) 생성
sess = tf.Session()
# 3단계 : 실제 실행 및 출력
print(sess.run(node1))
print(sess.run(node1).decode())

# 간단한 연산 tensor 그래프
# 1. 그래프 정의
node1 = tf.constant(10, dtype=tf.float16)
node2 = tf.constant(20, dtype=tf.float16)
node3 = tf.add(node1, node2)
# 2. Session 생성
sess = tf.Session()
# 3. Session 실행 & 결과
print(sess.run([node1, node2, node3]))
[10.0, 20.0, 30.0]

# 타입 변경
node1 = tf.constant(np.array([1,2,3]), dtype=tf.int16)
node2 = tf.cast(node1, dtype=tf.float32)
sess = tf.Session()
print(sess.run([node1, node2]))
[array([1, 2, 3], dtype=int16), array([1., 2., 3.], dtype=float32)]

# 평균값 계싼 : tf.reduce_mean()
data = np.array([1.,2.,3.,4.])
m = tf.reduce_mean(data)
sess = tf.Session()
print(sess.run(m))
2.5

# tf.random_normal([n]) : 평균이 0이고 표준편차가 1인 난수 n개 발생
w = tf.random_normal([3]) 
sess = tf.Session()
sess.run(w)
array([-0.6306145 ,  0.21123382, -0.35345677], dtype=float32)

# 변수 node
w = tf.Variable(tf.random_normal([1]))
sess = tf.Session()
sess.run(tf.global_variables_initializer()) # 변수 초기화 : 이것을 실행해야 상기 난수 1개가 w에 변수로 들어감
sess.run(w)

# predict를 위한 placeholder 이용
placeholder : 외부에서 데이터를 입력받을 수 있는 node
x = tf.placeholder(dtype=tf.float32)
H = 1 * x + 1
sess = tf.Session()
sess.run([x, H], feed_dict={x : np.array([40,50])})
[array([40., 50.], dtype=float32), array([41., 51.], dtype=float32)]

# Scale 조정
scale이 다른 데이터들의 회귀분석 구현(scale 조정 O)
scale 조정 방법 : 모든 데이터를 일정 범위 내로 조정
normalization(정규화) : 모든 데이터를 0~1 사이로 조정 
normalization = X - Xmin / Xmax - Xmin
  * 위의 식보다 라이브러리 추천 (sklearn.preprocessing.MinMaxScaler)
standardization(표준화) : 데이터의 평균을 0, 표준편차를 1로 조정
standardization = X - Xmean / Xstd
  * 위의 식보다 라이브러리 추천 (sklearn.preprocessing.StandardScaler)

# 라이브러리 쓰지 않고 정규화
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
# X_data.min(), X_data.max(), X_data.mean(), X_data.std()
norm_scaled_x_Data = (x_Data - min(x_Data)) / (max(x_Data) - min(x_Data))
norm_scaled_y_Data = (y_Data - min(y_Data)) / (max(y_Data) - min(y_Data))
norm_scaled_x_Data, norm_scaled_y_Data
(array([0.        , 0.11111111, 0.44444444, 0.77777778, 1.        ]),
 array([0.        , 0.11111111, 0.7       , 0.83333333, 1.        ]))

# 라이브러리를 사용하여 정규화 : 라이브러리에 넣기 위해서는 n행 1열로 reshape 필요!
x_Data = x_Data.reshape(-1, 1)
y_Data = y_Data.reshape(-1, 1)
from sklearn.preprocessing import MinMaxScaler # class
scaler_x = MinMaxScaler() # 독립변수 x를 정규화시킬 객체
scaler_x.fit(x_Data)
norm_scaled_x_Data = scaler_x.transform(x_Data)
scaler_y = MinMaxScaler() # 종속변수 y를 정규화시킬 객체
norm_scaled_y_Data = scaler_y.fit_transform(y_Data)
norm_scaled_x_Data, norm_scaled_y_Data
(array([[0.        ],
        [0.11111111],
        [0.44444444],
        [0.77777778],
        [1.        ]]),
 array([[0.        ],
        [0.11111111],
        [0.7       ],
        [0.83333333],
        [1.        ]]))

# 라이브러리 쓰지 않고 표준화
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
stan_scaled_x_Data = (x_Data - x_Data.mean()) / x_Data.std()
stan_scaled_y_Data = (y_Data - y_Data.mean()) / y_Data.std()
np.column_stack([x_Data, norm_scaled_x_Data, stan_scaled_x_Data])
array([[ 1.        ,  0.        , -1.22474487],
       [ 2.        ,  0.11111111, -0.93313895],
       [ 5.        ,  0.44444444, -0.05832118],
       [ 8.        ,  0.77777778,  0.81649658],
       [10.        ,  1.        ,  1.39970842]])
np.column_stack([y_Data, norm_scaled_y_Data, stan_scaled_y_Data])
array([[ 5.        ,  0.        , -1.32373476],
       [15.        ,  0.11111111, -1.04563922],
       [68.        ,  0.7       ,  0.42826713],
       [80.        ,  0.83333333,  0.76198177],
       [95.        ,  1.        ,  1.17912508]])
# 라이브러리를 사용하여 표준화 : 라이브러리에 넣기 위해서는 n행 1열로 reshape 필요!
x_Data = x_Data.reshape(-1, 1)
y_Data = y_Data.reshape(-1, 1)
from sklearn.preprocessing import StandardScaler # class
scaler_x = StandardScaler() # 독립변수 x를 정규화시킬 객체
stan_scaled_x_Data = scaler_x.fit_transform(x_Data)
scaler_y = StandardScaler() # 종속변수 y를 정규화시킬 객체
stan_scaled_y_Data = scaler_y.fit_transform(y_Data)
stan_scaled_x_Data, stan_scaled_y_Data

# 스케일 조정된 데이터를 다시 복구
scaler_y.inverse_transform(stan_scaled_y_Data)
scaler_x.inverse_transform(stan_scaled_x_Data)


# scale이 다른 데이터들의 회귀분석 구현(scale 조정 후)
x_Data = np.array([1, 2, 5, 8, 10])
y_Data = np.array([5, 15, 68, 80, 95])
# placeholder 설정
x = tf.placeholder(dtype=tf.float32)
y = tf.placeholder(dtype=tf.float32)
# weight & bias
W = tf.Variable(tf.random_normal([1]))
b = tf.Variable(tf.random.normal([1]))
# Hypothesis
H = W * x + b
# cost(손실함수)
cost = tf.reduce_mean(tf.square(H-y))
# 경사하강법
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)
# Session 생성 및 변수 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())
# 학습(scale 조정 후)
for step in range(1, 6001):
    _, cost_val = sess.run([train, cost], feed_dict={x:norm_scaled_x_Data,
                                                     y:norm_scaled_y_Data})
    if step %300==0:
        print('{}/6000번째 cost:{}'.format(step, cost_val))