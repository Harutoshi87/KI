1절. 변수 선택과 차원 축소
1-1 변수선택과 차원축소
종속변수에 영향을 주는 변수들을 찾아 학습에 사용할 독립변수의 수를 줄임 (어떻게 하면 score를 높일 수 있을지?)
과적합과 변수들 사이의 다중공선성(변수들간 강한 상관관계)을 줄일 수 있음
회귀계수 해석이 어려워짐. 모델 예측력이 좋아도 해석력이 떨어짐(어떤 변수가 제일 큰 요인인지 잘), p값이 나 유의성 검정이 왜곡될 수 있음
모형의 학습 시간을 줄일 수 있음
주성분분석, 상관분석, 분류모형의 feature_importance_, 예측 모형의 coef_
SelectKBest : 가장 높은 score에 따라 K개의 특징을 선택
1-2 주성분분석(PCA, Principal Component Anaysis)
주성분분석은 변수 선택 및 차원축소 방법(기존의 모든 변수를 조합하여 새로운 변수로 만듦) 으로 널리 사용
주성분 분석은 상관관계가 있는 변수들을 선형결합해서 분산이 극대화된 상관관계가 없는 새로운 변수(주성분) 들로 축약하는 것
주성분 분석은 사실 선형대수학이라기보다는 선형대수학의 활용적인 측면이 강하며 영상인식, 통계 데이터분석 (주성분 찾기), 데이터 압축, 노이즈제거 등 여러 분야에 사용
영상처리에서 많이 활용 : 여러개의 영상 중 대표 이미지를 찾을 때 활용

2절. 파라미터 탐색
하이퍼파라미터(모델의 성능에 영향을 미칠 수 있는 사용자가 직접 설정하는 파라미터).
어떤 파라미터를 사용하는게 최적의 결과를 낼지 탐색
sklearn패키지의 하이퍼 파라미터 튜닝 도구
validation_curve() : 단일 하이퍼 파라미터 최적화 함수
GridSearchCV : 복수 하이퍼파리미터 최적화 클래스

3절. 자료 불균형 처리
단순 오버/언더 샘플링
단, 단순 오버샘플 시 소스의 데이터를 복사하면 그 데이터들에 의해 과적합이 생길 수 있음
대신 SMOTE 라이브러리를 이용한 오버샘플링

4절. ensemble 모형
목적 : 여러 개 분류 모델을 하나의 통합 분류 모델로 연결하여 개별 분류모델보다 더 좋은 성능 달성
방법 :
배깅(bagging) : 분류를 잘하는 모델에 가중치 (병렬 작업) ex. RandomForest
부스팅(boosting) : 분류가 안된 데이터에 가중치 (순차 작업) ex. XGBoost, LGBM, AdaBoost - 불균형 데이터에 유용
투표(voting) : 여러 개 모델을 다수결 투표 (시간이 가장 오래 걸림)