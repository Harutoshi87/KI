ch2.Ollama_LLM활용의 기본 개념(LangChain)

LangChain :
거대 언어 모형(LLM)을 프로그램의 핵심 판단 장치로 사용하여 
질문 형식, 대화 기록, 외부 기능, 자료 저장소, 실행 순서를 체계적으로 연결함으로써 
여러 단계에 걸친 사고와 복잡한 작업 처리를 가능하게 하는 개발 도구

1. LLM을 활용하여 답변 생성하기
	1) Ollama를 이용한 로컬 LLM 이용
	성능은 GPT, CLaude 같은 모델보다 떨어지나, 개념설명을 위해 open source 모델 사용

	ollama.com 다운로드 -> 설치 -> 모델 pull
	cmd창이나 powershell 창에 ollama pull deepseek-r1:1.5b https://docs.langchain.com/oss/python/integrations/chat/ollama

	모델 pull
	cmd창이나 powershell창(window키+R에서 powershell)에서 ollama pull llama3.2:1b

	2) openai 활용
	pip install langchain-openai
	https://auth.openai.com/log-in


2. 렝체인 스타일로 프롬프트 작성
프롬프트 : llm호출시 쓰는 질문

	1) 기본 프롬프트 템플릿 사용
	PromptTemplate을 사용하여 변수가 포함된 템플릿을 작성하면 PromptValue

	2) 메세지 기반 프롬프트 작성
	BaseMessage 리스트
	BaseMessage 상속받은 클래스 : AIMessage, HumanMessage, SystemMessage, ToolMessage
	vscode에서 ctrl+shift+p : python:select interpreter입력 -> python환경선택
	vscode에서 커널 선택

	3) ChatPromptTemplate 사용
	BaseMessage리스트 -> 튜플리스트


3. 답변 형식 컨트롤하기
llm.invoke()의 결과는 AIMessage() -> string이나 json, 객체 : OutputParser이용
	1) 문자열 출력 파서 이용
	StrOutputParser를 사용하여 LLM출력(AIMessage)을 단순 문자열로 변환

	2)Json 출력 파서 이용 : JsonOutputParser()
	{'name':'홍길동', 'age':22} 

	3) 구조화된 출력 사용
	Pydantic : 데이터유효성검사, 설정관리를 간편하게 해주는 라이브러리
	Pydantic 모델을 사용하여 LLM출력을 구조화된 형식으로 받기(JsonParser보다 훨씬 안정적)


4. LCEL을 활용한 렝체인 생성하기
LCEL : LangChain Expression Language
LangChain에서 “체인(Chain)”을 더 간단하고 선언적으로 작성하기 위한 문법/표현식 언어
	1) 문자열 출력 파서 사용
	invoke : Runnable에 있는 함수
	StrOutputParser, ChatOllama, PromptTemplate등은 모두 Runnable로부터 상속 받음.

	2) LCEL을 사용한 간단한 체인 구성
	파이프연산자(|) 이용

	3) 복합체인 구성
	여러 단계의 추론이 필요한 경우(체인 연결)


















Pydantic : 데이터유효성검사, 설정관리를 간편하게 해주는 라이브러리